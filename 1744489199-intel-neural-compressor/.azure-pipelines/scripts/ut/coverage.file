[run]
branch = True

[report]
omit =
 */**/fake*yaml
 */**/fake.py
 */neural_compressor/model/nets_factory.py
 */neural_compressor/benchmark.py
 */neural_compressor/experimental/benchmark.py
 */neural_compressor/contrib/strategy/tpe.py
 */intel_extension_for_transformers/backends/*
 */intel_extension_for_transformers/optimization/utils/get_throughput.py
 */neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_decomposed_in.py
 */neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_in.py
 */neural_compressor/adaptor/tf_utils/graph_rewriter/int8/freeze_value.py
 */neural_compressor/template/*
 */neural_compressor/common/*
 */neural_compressor/torch/*
 */neural_compressor/tensorflow/*
exclude_lines =
 pragma: no cover
 raise NotImplementedError
 raise TypeError
 if self.device == "gpu":
 if device == "gpu":
 except ImportError:
 except Exception as e:
 onnx_version < ONNX18_VERSION
 onnx_version >= ONNX18_VERSION
